---
Title: "German Credit Data Analysis"
Author: "Srisai Sivakumar"
Date: "Friday, July 24, 2015"
Output: html_document
---

# German Credit Data Analysis

## By: Srisai Sivakumar

### Introduction

When a bank receives a loan application, based on the applicant's profile the bank has to make a decision regarding whether to go ahead with the loan approval or not. Two types of risks are associated with the bank's decision:

- If the applicant is a good credit risk, i.e. is likely to repay the loan, then not approving the loan to the person results in a loss of business to the bank

- If the applicant is a bad credit risk, i.e. is not likely to repay the loan, then approving the loan to the person results in a financial loss to the bank

It may be assumed that the second risk is a greater risk, as the bank (or any other institution lending the money to a untrustworthy party) had a higher chance of not being paid back the borrowed amount.

So its on the part of the bank or other lending authority to evaluate the risks associated with lending money to a customer.

This study aims at addressing this problem by using the applicant's demographic and socio-economic profiles to assess the risk of lending loan to the customer.

In business terms, we try to minimize the risk and maximize of profit for the bank. To minimize loss from the bank's perspective, the bank needs a decision rule regarding who to give approval of the loan and who not to. An applicant's demographic and socio-economic profiles are considered by loan managers before a decision is taken regarding his/her loan application.

The models used in thie study is not optimized for best performance. All models use the same 10-fold cross validation resampling option, without any manual tuning. Such manual tuning have been covered elsewhere in previous sections. Here we focus on the use of the penalty matrix as a loss function while building models.


### The data set

The German Credit data set is a publically available data set downloaded from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29)

All the details about the data is available in the above link. So we wont be describing the variables here.

The data contains data on 20 variables and the classification whether an applicant is considered a Good or a Bad credit risk for 1000 loan applicants.


```{r data,echo=FALSE,message=FALSE,warning=FALSE}

setwd("C:/Users/S.Srisai/Documents/working_directory/R/datasets/Statlog_(German_Credit_Data)_Data_Set")
data = read.table("german.data")
german.names=c("account.status","months","credit.history","purpose",
"credit.amount","savings","employment","installment.rate",              "personal.status","guarantors","residence","property","age","other.installments","housing","credit.cards","job","dependents","phone","foreign.worker","credit.rating")

names(data) = german.names

print("Lets look at the variables in the data set")

names(data)

data$credit.rating=as.factor(ifelse(data$credit.rating==1,"good","bad"))

data$credit.rating <- relevel(data$credit.rating, "good")

print("Lets look at the structure of the data")

str(data)

```


### Exploratory Analysis

Now that we have the data, its important that we understand the data before we attempt to model it.

Lets look at some descriptive stats of some of some of the numeric variables like months, age and credit amount.

```{r desc_stat,echo=FALSE,warning=FALSE,message=FALSE}

table(data$credit.rating)

table(data$credit.rating)/nrow(data)

summary(data$months)

summary(data$age)

summary(data$credit.amount)

```


Lets get a bit more statsy and look at cross-tabulation and test independence of the row and column variable.

```{r stats,echo=FALSE,warning=FALSE,message=FALSE}
library(gmodels)

with(data,CrossTable(credit.rating, savings, digits=1, prop.r=F, prop.t=F, prop.chisq=F, chisq=T))

with(data,CrossTable(credit.rating, personal.status, digits=1, prop.r=F, prop.t=F, prop.chisq=F, chisq=T))

with(data,CrossTable(credit.rating, dependents, digits=1, prop.r=F, prop.t=F, prop.chisq=F, chisq=T))
```

This reveals that there is dependence of savings and personal status on the credit rating. It also reveals that the number of dependents does not seem to have any bearing on the credit rating. Perhaps its fair to say that people who are intent on having a good credit rating continue to maintain the status irrespective of the number of dependents.

```{r stat.plots,echo=FALSE,warning=FALSE,message=FALSE}

brk <- seq(0, 80, 10)
hist(data$months, breaks=brk, xlab = "Credit Month", ylab = "Frequency", main = "Freqency of Credit Months ", cex=0.4,col='lightblue') 
grid(col = "lightgray", lty = "dotted")

hist(data$age, xlab = "Age", ylab = "Frequency", main = "Age Distribution", cex=0.4,col='lightblue')
grid(col = "lightgray", lty = "dotted")


hist(data$credit.amount, xlab = "Credit Amount", ylab = "Frequency", main = "Credit Amount Distribution", cex=0.4,col='lightblue')
grid(col = "lightgray", lty = "dotted")

```


```{r relationplot,echo=FALSE,message=FALSE,warning=FALSE}
library(lattice)
xyplot(credit.amount ~ age|purpose, data, grid = TRUE, group = credit.rating,auto.key = list(points = FALSE, rectangles = TRUE, space = "right"),main="Age vs credit amount for various purposes")

xyplot(credit.amount ~ age|personal.status, data, grid = TRUE, group = credit.rating,auto.key = list(points = FALSE, rectangles = TRUE, space = "right"),main="Age vs credit amount for Personal Status and Sex")

histogram(credit.amount ~ age | personal.status, data = data, xlab = "Age",main="Distribution of Age and Personal status & sex")

```

The first plot shows that the the most of the loans are sought to buy:

1. new car

2. furniture/equipment

3. radio/television

It also reveals that surprisingly few people buying used cars have bad rating! And not surprisingly, lower the age of the lonee and higher loan amount correlates to bad credits

The first obvious observation in the second plot is the absence of data for single women. Its not sure if its lack of data or there were no single women applying for loans- though the second possibility seems unlikely in real life. 

It reveal that single males tend to borrow more, and as before, younger they are and higher the loan amount corresponds to a bad rating.

The next most borrowing category is Female : divorced/separated/married.  The dominant trend in this category is smaller loan amount, higher the age, better the credit rating.

Males, married/widowed or divorced/separated have shown the least amount of borrowing. Because of this, its difficult to  visually observe any trends in these categories.

The histogram reveals that there is a right skewed nearly normal trend seen across all Personal Status and Sex categories, with 30 being the age where people in the sample seem to be borrowing the most.



### Models

Lets set up parallel processing.

```{r pp,echo=FALSE,message=FALSE,warning=FALSE}
library(doParallel)
x = detectCores()
cl<-makeCluster(x)
registerDoParallel(cl)
print(paste0("Number of registered cores is ",x))
```

We use the caret package to split the data into training and test sets with a 70/30 split.

```{r split,echo=FALSE,message=FALSE,warning=FALSE}
library(caret)
set.seed(1)
inTraining <- createDataPartition(data$credit.rating, p = .7, list = FALSE)
train <- data[ inTraining,]
test  <- data[-inTraining,]
str(train)
```

#### Penalty Matrix

An important aspect of most machine learning/predictive analytics problem this prediction accuracy. This problem too, is focussing on prediction accuracy. But thats not the only parameter we are concerned about. Recollect the objective of the study- 
1. minimize the risk, which would be ensured by good prediction accuracy
2. maximize the revenue to the bank, which is ensured by prompt repayment of the loans.

To achieve both, we not only have to ensure good prediction accuracy of good and bad credit ratings, we also need to minimize the mis-classification of good ratings as bad, and vice versa. It has to be noted that these 2 misclassifications DO NOT give same penalties. Its obvious that the bank stands more to lose if the customer with a bad rating is classified as a customer with a good rating, and given a loan that he/she defaults. This is more severe than classifying a good rating as bad, as it would make the bank miss only on the interest of the loan.

We achieve this goal by setting up a penalty matrix. Its shown below.

|         |           |           |           |
|---------|-----------|-----------|-----------|
|         | 1 = good  | Predicted | Predicted |
| 2 = bad | Penalties | 1         | 2         |
| Actual  | 1         | 0         | 1         |
| Actual  | 2         | 5         | 0         |

This means that we assign a penalty of 1 when we rate a customer as having bad credit raating, when infact the actual rating is good. This penalty signifies the loss of business to the bank.

We assign a much severe penalty of 5, when we misclassify a customer with bad rating, as having good rating. This is greater risk to the bank, hence higher penalty. To get the total penalty of a model, we mutiple each of the misclassification by the corresponding misclassification penalty score.

#### Baseline Models

As with any modelling task, we begin by defining the baseline model. Its with this model, we compare the models we are about to develop and establish improvements, if any.

The simplest prediction is taking all the loan applicants as having good credit rating. This would mean that we have about 70% accuracy. This doesnt mean anything without context. Lets compare this with the models we build and also with some literature.

This baseline would result in mis-classification that will be multiplied by the corresponding penalty score to get the total penalty of the model. Lets look at the accuracy and mean penalties of Baseline1 model

```{r base1,echo=FALSE,warning=FALSE,message=FALSE}
accu_base1 = mean(test$credit.rating=='good')
accu_base1
table(test$credit.rating,c(rep("good",nrow(test)),rep("bad",000)))
tot.penalty_base1 = table(test$credit.rating,c(rep("good",nrow(test))
                    ,rep("bad",000)))[1]*5
mean.penalty_base1 = table(test$credit.rating,c(rep("good",nrow(test))
                    ,rep("bad",000)))[1]*5/nrow(test)
metric_base1 = accu_base1/mean.penalty_base1
B1 = c(accu_base1,mean.penalty_base1)

print(paste0("Baseline-1 model has an accuracy of ",B1[1]," and a mean penalty of ",B1[2]))
```

Without any other reference, its difficult to say if this accuracy and penalty is good or not good. But we can certainly say that all the penalties here are due to classifying the bad credits as good (as we have assumed all the credits are good for baseline 1)

Lets look at the accuracy and mean penalty for Baseline 2 model.

```{r base2,echo=FALSE,message=FALSE,warning=FALSE}

accu_base2 = mean(test$credit.rating=='bad')
accu_base2
table(test$credit.rating,c(rep("good",0),rep("bad",nrow(test))))
tot.penalty_base2 = table(test$credit.rating,c(rep("good",0)
                   ,rep("bad",nrow(test))))[2]*1
mean.penalty_base2 = table(test$credit.rating,c(rep("good",0)
                   ,rep("bad",nrow(test))))[2]*1/nrow(test)
metric_base2 = accu_base2/mean.penalty_base2
B2 = c(accu_base2,mean.penalty_base2)

print(paste0("Baseline-2 model has an accuracy of ",B2[1]," and a mean penalty of ",B2[2]))

```

This model does far worse in prediction accuracy, simultaneously much better in mean penalty. All the penalties in this model is due to classifying good credit ratings as bad and the resulting loff of business to the bank.


Before we build the models, lets define the important Penalty Matrix, that would form the backbone of the analysis.

```{r penalty,echo=FALSE,warning=FALSE,message=FALSE}

PenaltyMatrix = matrix(c(0,1,5,0), byrow=TRUE, nrow=2)
PenaltyMatrix

```

Lets build few tree and random forest models and compare them with these 2 baseline models.

#### Trees

```{r tree,echo=FALSE,message=FALSE,warning=FALSE}
fitControl <- trainControl(method ="cv",number = 10)
tree = train(x=train[,-21],y=train[,21],method='rpart',
             trControl=fitControl)
tree
tree.pred = predict(tree,test[,-21])
confusionMatrix(test$credit.rating,tree.pred)
accu_tree=round(confusionMatrix(test$credit.rating,tree.pred)$overall[[1]],4)
tree.penalty.mat = as.matrix(table(test$credit.rating,tree.pred))*PenaltyMatrix
tot.penalty_tree = sum(as.matrix(table(test$credit.rating,tree.pred))*PenaltyMatrix)
mean.penalty_tree = sum(as.matrix(table(test$credit.rating,tree.pred))*PenaltyMatrix)/nrow(test)
metric_tree = accu_tree/mean.penalty_tree
T1 = c(accu_tree,mean.penalty_tree)

print(paste0("Tree model, tree, has an accuracy of ",T1[1]," and a mean penalty of ",round(T1[2],2)))
```


Lets add the penalty matrix into the tree model and see if that decreases the mean penalty.


```{r tree1,echo=FALSE,message=FALSE,warning=FALSE}
tree_p = train(x=train[,-21],y=train[,21],method='rpart',
             trControl=fitControl,parms=list(loss=PenaltyMatrix),maximize = FALSE)
tree_p
tree_p.pred = predict(tree_p,test[,-21])
confusionMatrix(test$credit.rating,tree_p.pred)
accu_tree_p=round(confusionMatrix(test$credit.rating,tree_p.pred)$overall[[1]],4)
tree_p.penalty.mat = as.matrix(table(test$credit.rating,tree_p.pred))*PenaltyMatrix
tot.penalty_tree_p = sum(as.matrix(table(test$credit.rating,tree_p.pred))*PenaltyMatrix)
mean.penalty_tree_p = sum(as.matrix(table(test$credit.rating,tree_p.pred))*PenaltyMatrix)/nrow(test)
metric_tree_p = accu_tree_p/mean.penalty_tree_p
T2 = c(accu_tree_p,mean.penalty_tree_p)

print(paste0("Tree model,tree_p, has an accuracy of ",T2[1]," and a mean penalty of ",round(T2[2],2)))
```

We see that with the inclusion of the penalty matrix, the mean penalty has reduced.

#### Random Forest

```{r rf,echo=FALSE,warning=FALSE,message=FALSE}
set.seed(3000)
rf = train(x=train[,-21],y=train[,21],method='rf',do.trace=F,
           allowParallel=T,trControl=fitControl)
rf
rf.pred = predict(rf,test[,-21])
confusionMatrix(test$credit.rating,rf.pred)
accu_rf = round(confusionMatrix(test$credit.rating,rf.pred)$overall[[1]],4)
table(test$credit.rating,rf.pred)
rf.penalty.mat = as.matrix(table(test$credit.rating,rf.pred))*PenaltyMatrix
tot.penalty_rf = sum(as.matrix(table(test$credit.rating,rf.pred))*PenaltyMatrix)
mean.penalty_rf = sum(as.matrix(table(test$credit.rating,rf.pred))*PenaltyMatrix)/nrow(test)
metric_rf = accu_rf/mean.penalty_rf
R1 = c(accu_rf,mean.penalty_rf)

print(paste0("Random Forest model, rf, has an accuracy of ",R1[1]," and a mean penalty of ",round(R1[2],2)))
```

Lets add the penalty matrix to the model

```{r rf1, echo=FALSE,message=FALSE,warning=FALSE}
set.seed(3000)
rf_p = train(x=train[,-21],y=train[,21],method='rf',do.trace=F,
             allowParallel=T,trControl=fitControl, importance = T,
             parms=list(loss=PenaltyMatrix),maximize = FALSE)
rf_p
rf_p.pred = predict(rf_p,test[,-21])
confusionMatrix(test$credit.rating,rf_p.pred)
accu_rf_p = round(confusionMatrix(test$credit.rating,rf_p.pred)$overall[[1]],4)
table(test$credit.rating,rf_p.pred)
rf_p.penalty.mat = as.matrix(table(test$credit.rating,rf_p.pred))*PenaltyMatrix
tot.penalty_rf_p = sum(as.matrix(table(test$credit.rating,rf_p.pred))*PenaltyMatrix)
mean.penalty_rf_p = sum(as.matrix(table(test$credit.rating,rf_p.pred))*PenaltyMatrix)/nrow(test)
metric_rf_p = accu_rf_p/mean.penalty_rf_p
R2 = c(accu_rf_p,mean.penalty_rf_p)

print(paste0("Random Forest model, rf_p, has an accuracy of ",R2[1]," and a mean penalty of ",round(R2[2],2)))

rf1Imp <- varImp(rf_p, scale = FALSE)
plot(rf1Imp, top = 10,main="10 most important predictor for prediction of credit rating")

```

We see that with the inclusion of the penalty matrix, the mean penalty has gone down.

It can be noted that the most important predictor of the credit rating is account status, followed by months and credit history

### Results

We have the models we need, lets look at the results we have in hand. As we look at the models and its results, its important to analyze the results in the right context.

Besides accuracy and mean penalties, its important to have a metric that summarizes the effect of both accuracy and penalties. We can try the ratio of accuracy to penalty as the metric. But this skews the results. Instead, accuracy^2/penalty gives a better summary. So this will be used as the performance index metric for all the models developed above.


```{r results,echo=FALSE,warning=FALSE,message=FALSE}
result <- data.frame(model = c("Baseline-1","Baseline-1", "Tree","Tree1","Random.Forest" ,"Random.Forest.Penalty"),
accuracy = c(accu_base1,accu_base2,accu_tree,accu_tree_p,accu_rf,accu_rf_p),
penalty = c(mean.penalty_base1,mean.penalty_base2,mean.penalty_tree,mean.penalty_tree_p,mean.penalty_rf,mean.penalty_rf_p),
zeros = c(0,0,0,0,0,0))

result$index <- round(with(result,accuracy*accuracy/penalty),3)

print("Lets look at the accuracy of each model")

barplot((result$accuracy)*100, col = 1:6 ,main="Model Accuracy", 
        xlab="Models", ylab = "Accuracy (%)")
legend("bottom", legend = result$model, fill = 1:6, ncol = 2, cex = 0.8)
grid(col = "lightgray", lty = "dotted")

print("Lets look at the mean penalty of each model")

barplot(result$penalty, col = 1:6 ,main="Model Penalties",xlab="Models",
        ylab = "Penalty")
legend("topright",legend = result$model,fill = 1:6, ncol = 2, cex = 0.8)
grid(col = "lightgray", lty = "dotted")

print("Lets look at the Performance Index of each model")

barplot(result$index, col = 1:6 ,main="Model Performance",xlab="Models",
        ylab = "Performance Index (accuracy^2/mean.penalty)")
legend("topleft",legend = result$model, fill = 1:6, ncol = 2, cex = 0.8)
grid(col = "lightgray", lty = "dotted")

print("Lets look at the accuracy-penalty summary of each model")

plot(c(B1[1],0),c(0,B1[2]),type='l',col=1,lwd=3,
     xlab="Accuracy of the model",
     ylab="Penalty of the model",
     main="Accuracy-Penalty Comparison",
     xlim=c(0.1,1),ylim=c(0.1,2))
lines(c(B2[1],0),c(0,B2[2]),type='l',col=2,lwd=3)
lines(c(T1[1],0),c(0,R1[2]),type='l',col=3,lwd=3)
lines(c(T2[1],0),c(0,R2[2]),type='l',col=4,lwd=3)
lines(c(R1[1],0),c(0,R1[2]),type='l',col=5,lwd=3)
lines(c(R2[1],0),c(0,R2[2]),type='l',col=6,lwd=3)
legend("topright",legend =  c("Baseline-1","Baseline-2","Tree","Tree-Penalty","RF","RF-Penalty"),cex=0.75, col = c(1,2,3,4,5,6),
       lwd=c(3,3,3,3,3,3)) 
grid(col = "lightgray", lty = "dotted")

```

The penalties plot clearly shows the decrease of mean penalty. This affirms we have progressed in the right direction.

The model performance index plots also reveal that with better models, we have achieved better performance index values.

### Conclusion

We have modelled the German Credit Data set using naive and simple baseline models to random forest models. We have improved the from 0.7, to 0.76 with the r_f_p model. 

Results of the same data set available [elsewhere](http://bayesian-intelligence.com/publications/TR2010_1_zonneveldt_korb_nicholson_bn_class_credit_data.pdf) shows similar order of accuracies for prediction. Results from Applications of Data Mining in E-business and Finance, pp 28 also gives similar accuracies. So our model has similar prediction accuracies to the ones in literature.

We have also learnt how to apply penalty matrix to the tree and random forest models. We have seen that this reduces unfavourable mis-classifications


Footnote: If the file fails to get converted to html using the 'Knit HTML' option in R-Studio, we use the following lines of code to knit the file to html.
library(knitr)
knit2html("German.Rmd")
